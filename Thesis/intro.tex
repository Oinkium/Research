\chapter{Introduction}

\section{Denotational Semantics and Program Equivalence}

Given two pieces of computer code, in what circumstances can we say that they are interchangeable?
I.e., when can we be absolutely sure that we can replace one with the other and know that the behaviour of the program will not change?
This is a simple question with a complicated answer.

A starting point is to require that the two pieces of code should return the same output for any choice of input values. 
But that is not necessarily enough.
For example, the following two Haskell functions appear to do the same thing: indeed, both return $0$, whatever input is passed in.

\begin{lstlisting}[language=haskell]
f :: Int -> Int
f n = if (n == 0) then 0 else 0

g :: Int -> Int
g n = 0
\end{lstlisting}

However, if we introduce a non-terminating function
\begin{lstlisting}[language=haskell]
diverge :: Int -> Int
diverge x = diverge x
\end{lstlisting}
then it becomes clear that \lstinline[language=haskell]{f} and \lstinline[language=haskell]{g} are not interchangeable: indeed, since Haskell evaluates inputs to functions lazily, evaluating \lstinline[language=haskell]{g (diverge 0)} will not evaluate the non-terminating term \lstinline[language=haskell]{(diverge 0)} and will terminate at the value \lstinline[language=haskell]{0}, while \lstinline[language=haskell]{f (diverge 0)} \emph{will} evaluate \lstinline[language=haskell]{(diverge 0)} and will itself fail to converge.

This is not itself a difficult problem to get around: all we have to do is treat non-termination as a separate input value in itself.
Thus, we add to each datatype an extra distinguished value $\bot$ representing non-termination (so, for example, the type of integers is represented by the set $\bZ_\bot = \bZ + \{\bot\}$), and then function types are interpreted as functions between these sets -- so a term of type \lstinline[language=haskell]{Int -> Int} is represented by a function $\bZ_\bot \to \bZ_\bot$.
We can then tell that our functions \lstinline[language=haskell]{f} and \lstinline[language=haskell]{g} are different, since \lstinline[language=haskell]{g} corresponds to the constant $0$ function $\bZ_\bot \to \bZ_\bot$, while \lstinline[language=haskell]{f} corresponds to the function that sends $n\in \bZ$ to $0$ but sends $\bot$ to $\bot$.

Every program gives rise to a function in this way, but not every such function arises from a program.
For example, we cannot write a program corresponding to the function $\chi\from\bZ_\bot \to \bZ_\bot$ that sends $\bot$ to $0$ and all other values to $1$: since $\chi$ is not constant, such a program would have to evaluate its argument, and would consequently fail to terminate if that argument did not terminate, so it would have to send $\bot$ to $\bot$.

If our language admits higher types, then it becomes especially important to exclude such `impossible' functions from our model.
For example, if $F$ and $G$ are two programs of type \lstinline[language=haskell]{(Int -> Int) -> Int} -- i.e., functions that take in a function from integers to integers and return an integer -- then we do not want to declare that $F$ and $G$ are different on the basis that $F(\chi)\ne G(\chi)$.  

In order to rule out these `impossible' functions, we define a partial order on the sets $T_\bot$ corresponding to types, defined by setting $x\le x$ and $\bot\le x$ for all $x$.
We then require that functions should be monotonic and continuous with respect to this order. 
For example, a monotonic function $\bZ_\bot \to \bZ_\bot$ is either constant (corresponding to a function that does not evaluate its argument at all) or sends $\bot$ to $\bot$ and is otherwise unconstrained (corresponding to a function that evaluates its argument).
It turns out that if we order functions pointwise, then we get the correct constraints at higher types as well.

Even if we get round the problems with divergence, there are other language features that we may need to consider if we want to determine whether two pieces of code are equivalent. 
If our functions have access to global variables, then we need to check that these variables end up taking the same final value, whatever their initial values were.
If we have IO calls in our language, then we need to check that the functions print out the same text, whatever the user input was.
If we have a random number generator, then we need to check that our functions return the same \emph{set} of values, whatever the input.

What we have been doing in all these examples is \emph{denotational semantics}: the art of using mathematical objects to study logic and programming languages.  
In the first case, our denotational semantics was expressed through the mathematics of sets and functions, where we captured the behaviour of a (programming language) function via a (mathematical) function.

Then, following Scott \cite{ScottDomains}, we refined this model to one based on partially ordered sets -- more specifically, \emph{Scott domains} -- in which we modelled the behaviour or a program via an associated Scott-continuous functions between domains.

In our other examples, we need to come up with further refinements to our model in order to incorporate the new computational effects. 
For example, to handle nondeterminism, we might want to switch to using nondeterministic functions, or \emph{relations}, instead of ordinary functions.

The advantages in all of these cases is that the mathematical objects we use are often fairly simple, whereas computer programs, even in simple `toy' languages, are very complicated to study. 
A program is, at its heart, a string of symbols governed by a collection of operational rules that govern how such strings should behave. 
Such an object is very fiddly to reason with directly; indeed, the only way to think about it is as some kind of `function' from input to outputs.
Denotational takes this basic intuition further, and aims to capture features of programming languages through a diverse collection of different mathematical models.

A word of warning: the principal mode of denotational semantics which we shall be studying in this thesis is game semantics, which is much more complicated than the semantics of sets and functions. 

\section{Computational Adequacy \& Full Abstraction}
\label{SecCAnFA}

In order for a denotational semantics to tell us anything, we first need to prove some results that relate it to the language we are studying. 
For example, if we are hoping to model a programming language using sets and functions, then we need to define a mapping $\deno{-}$ (the \emph{denotation}) that takes program types to sets and program functions to functions between those sets, and we also need to prove that this denotation respects the operational rules of the language.  
For example, we might want to prove that if $f \from \Int \to \Int$ is a function and $M \from \Int$ is a term that evaluates to the integer $n$, then the term $f M$ will evaluate to the integer $\deno{f}(n)$.

This type of result is called \emph{Computational Adequacy}, and relates to a program's \emph{observable behaviour}.  
Some programs have an automatic notion of observable behaviour: if we run them, then they will return a concrete value or fail to terminate.  
In a typed setting, these are the programs of ground type.
Others -- for example, programs of function type -- require input before they can return a value.  
These programs have no observable behaviour of their own, but if we insert them into a hole in a larger context, then they may influence the observable behaviour of the complete program.

Briefly speaking, a Computational Adequacy result tells us that the observable behaviour of a program of ground type may be deduced exactly from its denotation.
For example, in a domain-theoretic semantics, we might want to say that a program $M$ evaluates to a value $v$ if and only if $\deno M=v$ and that $M$ fails to terminate if and only if $\deno M = \bot$.

Such a computational adequacy result extends readily to terms not of ground type.  
Given two programs $M$ and $N$ of the same type, we say that $M$ and $N$ are \emph{observationally equivalent} if $C[M]$ and $C[N]$ have the same behaviour for any one-holed context $C[-]$ of ground type.
If our semantics is \emph{compositional} -- so that the denotation of $C[M]$ is obtained by `applying' the denotation of $C[-]$ to the denotation of $M$ -- and computationally adequate, then it follows that the semantics is \emph{equationally sound}: if two terms $M$ and $N$ have the same denotation, then they are observationally equivalent.

If we have an effective way of computing denotations, then this can give us an easy way to prove observational equivalence of terms.  
However, it gives no guarantee that this technique is always going to work: the terms $M$ and $N$ might be observationally equivalent despite having distinct denotations.  
The gold standard of denotational semantics -- \emph{Full Abstraction} -- asserts in addition that the converse of equational soundness holds, so that the denotational semantics completely captures the observational equivalence relation.  
This means that if two terms are observationally equivalent, then we can always prove that they are observationally equivalent by showing that they have the same denotation.

An important early success in this direction came with Plotkin's introduction of the stateless sequential programming language PCF \cite{PlotkinPcf}.  
Plotkin was unable to provide a fully abstract denotational semantics for PCF itself, but he showed that if we add a simple parallel construct\footnote{Specifically, `parallel or', which evaluates its two boolean arguments in parallel, and is thus able to return true if either the left or the right argument returns true, even if the other fails to terminate.} to PCF, then a denotational semantics based on Scott domains is fully abstract.  
This result presents us with a world in which we can practically and systematically check observational equivalence (for terms of this extended version of PCF) by computing denotations.  
If two term displays infinitary behaviour, then it is not necessarily possible to check whether their denotations are equal, but if they are finitely presentable in some sense, so that their denotations as functions between Scott domains can be computed and compared, then the Full Abstraction result gives us, at least in principle, a systematic recipe for checking observational equivalence.

Unfortunately, this stops working once we remove the extra parallel construct.
PCF is a sub-language of the parallelized version, and therefore can also be given a denotational semantics via Scott domains, but the absence of parallelism also means that the observational equivalence relation is coarser: there may be terms that can be distinguished by a context including the parallel construct that cannot be distinguished inside any purely sequential context.  
This means that the Full Abstraction result does not automatically pass over to sequential PCF.

Any hope of solving the problem with a tweaked model was brought down to earth by Ralph Loader's 2001 theorem that observational equivalence for PCF is undecidable, even if we restrict ourselves to a finitary version of the language with no infinite datatypes or recursion beyond a simple non-termination primitive $\bot$.
This in particular tells us that no concretely presentable denotational semantics for PCF can possibly be fully abstract, or it would in principle give us an algorithm for deciding observational equivalence in this finite version.

Despite this fact, there were, roughly contemporaneous with Loader's result, several fully abstract models of PCF published, in a watershed moment for the subject.
The model published by O'Hearn and Riecke \cite{OHearnRieckePcf} was more or less along domain-theoretic lines, while those of Abramsky, Jagadeesan and Malacaria \cite{ajmPcf}, Hyland and Ong \cite{hoPcf}, and Nickau \cite{NickauPcf} used the relatively new Game Semantics.

These models took a slightly oblique approach to Full Abstraction, which is how they reconcile themselves with Loader's Theorem.
First, they defined the notion of \emph{intrinsic equivalence} of terms of the same type $T$ in a denotational model, where two elements $\sigma$ and $\tau$ of the denotation of $T$ are intrinsically equivalent if $\alpha(\sigma)=\alpha(\tau)$ for all functions $\alpha\from\deno{T}\to\deno{o}$ from the denotation of $T$ to the denotation of some fixed ground type $o$.
This definition is very closely linked to that of observational equivalence; indeed, if two terms $M$ and $N$ are observationally equivalent, and we are working in a computationally adequate and compositional denotational semantics, then the denotations of $M$ and $N$ will be intrinsically equivalent, since for any ground-type context $C[-]$, we can take $\alpha$ to be the denotation of $C[-]$ in the above definition.

Proving the converse -- that intrinsic equivalence of denotations implies observational equivalence -- entails going in the opposite direction; i.e., starting with some element $\alpha$ in the model and coming up with a context $C[-]$ in the language whose denotation is $\alpha$.  
Thus, proving this direction normally reduces to some kind of \emph{definability} result.  
Typically, we do not actually need to prove that every $\alpha$ is definable within the language -- and, indeed, this is often not the case.  
Instead, we try to show that each element $\alpha$ in the language can be written as the least upper bound of elements $\alpha'$ living in some more restricted class whose members can all be defined in the language.  
This class usually consists of those elements which are \emph{compact} or finitary in some sense, though McCusker's Fully Abstract relational semantics for SCI \cite{GuySci} is an important exception.
Under mild continuity assumptions, this suffices to prove Full Abstraction, for we can then deduce that if $\alpha(\sigma)\ne \alpha(\tau)$ for some $\alpha$, then there is some definable $\alpha'$ such that $\alpha'(\sigma)\ne\alpha'(\tau)$.
This is the approach taken by the fully abstract semantics that have been given for PCF; there is no contradiction of Loader's theorem, because the intrinsic equivalence relation is itself undecidable, even for finitary terms.

If we can prove, for some denotational model of a language, that observational equivalence of terms is equivalent to intrinsic equivalence of their denotations, then we can form a fully abstract model by passing to equivalence classes under the intrinsic equivalence relation.  
In this thesis, we shall skip the final step of passing to equivalence classes and declare a denotational semantics to be fully abstract for a language if we can prove that observational equivalence of terms is equivalent to intrinsic equivalence of their denotations.  
Thus, the Full Abstraction results that we prove will have three main ingredients: compositionality, computational adequacy and definability.

Lastly, we note that the conclusion of Loader's theorem does not necessarily hold for other languages.  
For example, in Section \ref{SecIntrinsicEquivalence}, we shall demonstrate a denotational characterization of observational equivalence in Idealized Algol, due to Abramsky and McCusker \cite{SamsonGuyIAActive}, which can be adapted to give us an algorithm for deciding observational equivalence for a finitary version of Idealized Algol.

\section{Categorical Semantics}
\label{SecCatSem}

There is a close link between (typed) programming languages and categories.
Most programming languages have things called \emph{types}, and many have \emph{functions} that go from one type to another. 
Typically, it will be possible to compose two functions together in the language in an associative way, giving us a category.
It should come as no surprise, then, that a very important branch of denotational semantics is \emph{categorical semantics}, in which we take some existing category from mathematics, and use its objects and morphisms to represent the types and terms of a programming language.

Typically, each type $T$ of the language will correspond to some object $\deno T$ of the category, while a term of type $T$ will correspond to a morphism $1 \to \deno T$, where $1$ is some fixed object in the category (usually a terminal object).

Particularly important \cite{Lambek} are the Cartesian closed categories, which have a number of properties making them suitable for denotational semantics:
\begin{description}
  \item[Product and function spaces] Given types $S$ and $T$, we can define the denotations of the product type $S \times T$ and the function type $S \to T$ to be given by $\deno S \times \deno T$ and ${\deno T}^{\deno S}$.
  \item[Compositionality] Given types $S$ and $T$, and corresponding objects $\deno S$ and $\deno T$ of the category, we can define the denotation of the function type $S \to T$ to be given by the exponentiation ${\deno T}^{\deno S}$ as above.
    Then we automatically have a recipe for substituting a term of type $S$ into a function of type $S\to T$ via the canonical morphism
    \[
      {\deno T}^{\deno S} \times \deno S \to \deno T\,.
      \]
  \item[Abstraction] Given a morphism $\sigma \from A \times B \to C$, we may form a morphism $\Lambda(\sigma) \from A \to C^B$.
    This gives us the semantics for $\lambda$-abstraction, whereby we pass from a term-in-context
    \[
      \Gamma,x\from S \ts M \from T
      \]
    to the term-in-context
    \[
      \Gamma \ts \lambda x.M\from S \to T\,.
      \]
\end{description}
These rules allow us to build up a model of the simply-typed $\lambda$-calculus within any Cartesian closed category, which means we get a large part of the denotation (and the subsequent proof of Computational Adequacy) for free.  

This alone would be a good justification for using category theory in denotational semantics, but the benefits go further.  
The development of programming languages such as Haskell has been strongly influenced by category-theoretic concepts.  
For example, Moggi's 1991 observation \cite{Moggi} that monads on categories provide us with a way of modelling computational effects influenced work that led directly to the introduction of support for monads in Haskell \cite{Haskell}, where they have become the primary tool for abstracting out effectful computation.

Monads will be particularly important in this thesis, so it is worth dwelling on them a little further.  
A \emph{monad} on a category $\C$ is given by a functor $M\from \C \to \C$, together with natural transformations
\begin{mathpar}
  e\from\id_\C \Rightarrow M \and m\from M\circ M\Rightarrow M
\end{mathpar}
that endow $M$ with an algebraic structure.
One example is the non-empty powerset functor on the category of sets, together with the natural transformations given by
\begin{mathpar}
  \begin{IEEEeqnarraybox*}{rCcCl}
    e & \from & A & \to & \powerset(A) \\
    && a & \mapsto & \{a\}
  \end{IEEEeqnarraybox*}
  \and
  \begin{IEEEeqnarraybox*}{rCcCl}
    m & \from & \powerset(\powerset(A)) & \to & \powerset(A) \\
    && \mathcal A & \mapsto & \bigcup_{A \in \mathcal A} A\,.
  \end{IEEEeqnarraybox*}
\end{mathpar}
This powerset monad indicates some kind of nondeterministic choice between elements of $A$, particularly if we modify the construction to the \emph{non-empty powerset} functor $\powerset_+$.

Another example in the category of sets is the functor $A \mapsto A + \{\bot\}$, that appends an additional element on to a set.  
We have natural functions $A \to A + \{\bot\}$ and $A + \{\bot\} + \{\bot\} \to A + \{\bot\}$ that make this into a monad as well.
In the study of programming languages, this is often called the \emph{maybe monad}, because $A + \{\bot\}$ indicates an element of $A$ that may or may not be present (with the distinguished value $\bot$ indicating no value).

Given a monad $M$ on a category $\C$, we can form a new category $\Kl_M\C$ -- the \emph{Kleisli category} of $M$ -- whose objects are the objects of $\C$ and where a morphism from $A$ to $B$ is given by a morphism $A \to MB$ in $\C$.  
The monadic coherence gives us the correct notion of composition: given Kleisli morphisms $\sigma \from A \to MB$ and $\tau\from B \to MC$, we may compose them to give a morphism $A \to MC$ via the following formula.
\[
  A \xrightarrow{\sigma} MB \xrightarrow{M\tau} M M C \xrightarrow{m} MC
  \]
The Kleisli category of the powerset monad is the category of sets and relations, while the Kleisli category of the maybe monad is the category of sets and partial functions.

There are numerous other monads that can be used to model computational effects, such as the state monad and the exception monad.  
Work by Plotkin and Power \cite{PlotkinPower} makes this more precise, by studying monads that can be built up via algebraic operations and equations.  
For example, we might want to model nondeterministic choice on a set $A$ via an operation $\sum$ that takes in infinitely many elements of $A$ -- so $\sum a_i$ gives us a choice between the $a_i$.  
We then impose some axioms on this operation.
\begin{description}
  \item[Idempotence] If $a_i=a$ for all $i$, then $\sum a_i=a$;
  \item[Commutativity] $\sum a_i = \sum_{a_{\pi(i)}}$ for any permutation $\pi$; and
  \item[Associativity] $\sum_i (\sum_j a_{ij}) = \sum_{i,j} a_{ij}$.
\end{description}
This is an algebraic theory akin to the theory of groups, and its category of free algebras is isomorphic to the Kleisli category of the powerset monad.

\section{Game Semantics}

Game Semantics gives us a particularly fruitful categorical semantics for programming languages.  
The underlying idea is that a computer program behaves like a strategy for a two-player game, in that it needs to respond to arbitrary inputs (opponent moves) with its own behaviours (proponent moves).  
Thus, we represent a programming language type by an idealized game between two players $O$ and $P$\footnote{Another convention is to refer to player $O$ as $\forall{}$belard and player $P$ as $\exists{}$lo\"{i}se, so I will refer to player $O$ as `he' and player $P$ as `she' throughout.}, and represent a term of that type by a strategy for that game.

The power of game semantics comes from the fact, first noted by Blass in \cite{blassgames}, that certain natural operations on games correspond to some of the connectives of linear logic.  
For example, if $A$ and $B$ are two-player games, then we may form their \emph{tensor product} $A\tensor B$, which is played by running the games $A$ and $B$ together in parallel, with the opposing player $O$ allowed to switch between games when it is his turn.

A closely related construction takes games $A$ and $B$ and forms their \emph{linear implication} $A\implies B$, in which $B$ is played in parallel with the \emph{dual} of $A$, in which the roles of players $P$ and $O$ are swapped round.  
This time, player $P$ can choose to switch game when it is her turn.

The remarkable thing about the $\implies$ construction, first pointed out by Joyal in \cite{joyalgames}, is that if $A$, $B$ and $C$ are games, then we may compose a $P$-strategy for $A\implies B$ with a $P$-strategy for $B\implies C$ to get a $P$-strategy for $A\implies C$.

In order to form her strategy, player $P$ sets up a `scratchpad' consisting of the games $A\implies B$ and $B\implies C$ side by side.
Now suppose that player $O$ makes a move in $A\implies C$ that originally came from the game $C$.  
Then player $P$ treats this move as a move in $B\implies C$, and uses her strategy for that game to come up with a reply.  
If that reply is a move in $C$, then she plays it as her response.
Otherwise, if it is a move in $B$, she treats that move as an $O$-move in $A\implies B$, and therefore has a reply in $A\implies B$ according to her strategy for that game.  
Eventually, if she plays a move in $A$ or $C$, then that will be her reply in the composite strategy.
See Figure \ref{FigComposition} for an illustration.
\begin{figure}
  \[
    \begin{tikzcd}[arrows={no head, dotted, shorten=-1.5mm}, row sep=-1pt]
      A
        & B
          &[-20pt] B
            & C \\
      %
        & 
          &
            & \circ \arrow[d] \\
      %
        & 
          & 
            & \bullet \\
      %
        & 
          & 
            & \circ \arrow[dl] \\
      %
        & \circ \arrow[d]
          & \bullet
            & \\
      %
        & \bullet
          & \circ \arrow[d]
            & \\
      %
        & \circ \arrow[dl]
          & \bullet
            & \\
      \bullet
        &
          &
            & \\
      \circ \arrow[dr]
        &
          &
            & \\
      %
        & \bullet
          & \circ \arrow[dr]
            & \\
      %
        &
          &
            & \bullet
    \end{tikzcd}
    \]
  \caption[Illustration of the composition of strategies in Game Semantics.]{Illustration of the composition of strategies in Game Semantics.  
  Working top-to-bottom, the symbol $\circ$ denotes a move by $O$ and the symbol $\bullet$ a move by $P$.  
  A dotted line indicates that a move is determined by one of player $P$'s strategies for the games $A\implies B$ and $B\implies C$.  
  Note that the moves in $B$ are duplicated, so that they may always be considered as an $O$-move in either $A\implies B$ or $B\implies C$.  
  Moves from $B$ are hidden in the composite strategy: in this case, player $P$'s first `real' move is in $C$, her second in $A$ and her third in $C$.}
  \label{FigComposition}
\end{figure}

It is possible instead that player $P$ flips between her two strategies for ever, always playing moves in $B$ and never coming out into $A$ or $C$.  
Computationally, this represents `livelock': a computation that does not terminate because two subroutines are continually deferring to each other without returning values of their own.

This composition of strategies gives us a category $\G$ in which the objects are games and a morphism from a game $A$ to a game $B$ is a strategy for the game $A\implies B$.  
In this category, the identity morphism on a game $A$ is the \emph{copycat strategy} for $A\implies A$, which responds to an $O$-move in either copy of $A$ with the identical move in the other copy.

What is more, the connectives $\tensor$ and $\implies$ make $\G$ into a symmetric monoidal closed category.  
We typically apply some category-theoretic construction to $\G$ to get a Cartesian closed category, by passing either to the Kleisli category for a linear exponential comonad on $\G$ (as in \cite{ajmPcf}; see \cite{SchalkWhatIs}), or to a subcategory of the category of cocommutative comonoids in $\G$ (as in \cite{hoPcf,SamsonGuyIAActive}; see \cite[\sec 3.5.2]{RusssThesis}).
Once we have a Cartesian closed category, we automatically have a way to interpret the simply-typed lambda calculus.

\section{Game Semantics for Programming Languages}

The first triumph of Game Semantics was to solve the Full Abstraction problem for PCF, but a more lasting application of the discipline has been to model more general programming languages with effects such as state.
By making changes to the definitions of game and strategy, Game Semantics has proved to be applicable to a wide variety of programming language effects, including exceptions \cite{LLi}, coroutines and continuations \cite{FunctionalProgramsAsCoroutines}, nondeterminism \cite{mcCHFiniteND}, probability \cite{DanosHarmer}, and general references \cite{HondaMcCusker}.

The precise definition of a strategy that we use depends on the language that we are trying to model -- a language with less expressive power can realize fewer strategies.  
For example, the denotations of terms in a stateless language such as PCF are \emph{history-free} or \emph{innocent}, in which the proponent's moves can only depend on a particular subsequence of the current sequence of moves -- the $P$-view -- rather than on the whole sequence.
So for a lot of computational effects, particularly ones that have something to do with state, adding that effect corresponds to a \emph{relaxing} of conditions on strategies.

We model other types of effects by extending the definition of a strategy.  
For example, if we want to provide a semantics for a language with nondeterminism, then we modify the definition of a strategy so that the proponent can have multiple replies to each opponent move, as in the work of Harmer and McCusker \cite{mcCHFiniteND}.
If we want to model a probabilistic language, then we decorate these different moves with probabilities, as in the work of Danos and Harmer \cite{DanosHarmer}.

When choosing a definition of a strategy, the aim is to prove a definability result, so that we can prove Full Abstraction.  
The original proofs of definability of compact innocent strategies in PCF from \cite{ajmPcf} and \cite{hoPcf} were intricate and technical.  
Subsequent work on languages that extend PCF tends to try to prove definability via a \emph{factorization result}, in which we show that every strategy in an extended category of games may be written as the composition of a strategy in an original category of games with some fixed strategy in the new category.  
Then, if we have a definability result for the original semantics, we can extend it to a definability result in the new category.

For example, the language Idealized Algol is an extended version of PCF that adds some stateful primitives.
Abramsky and McCusker's proof of compact definability for Idealized Algol in \cite{SamsonGuyIAActive} first proves that each compact strategy in their model may be written as the composite of a compact innocent strategy with the (non-innocent) denotation of one of the new stateful constants.  
Thus, they can deduce compact definability for Idealized Algol from Hyland and Ong's result that every compact innocent strategy is the denotation of a term of PCF.  

Similarly, Harmer and McCusker develop in \cite{mcCHFiniteND} a model of game semantics in which strategies can be nondeterministic.  
They show that every such strategy can be written as the composite of a deterministic strategy with some particular fixed nondeterministic strategy.  
Then, to prove compact definability for nondeterministic Idealized Algol, it suffices for them to exhibit a nondeterministic term whose denotation is that fixed strategy.

We can now summarize the typical process of proving a Full Abstraction language for a language $\L'$ that extends a language $\L$ as follows.
\begin{itemize}
  \item Starting with an existing computationally adequate model $\C$ of $\L$ that satisfies compact definability, define a categorical model $\C'$ that admits an identity-on-objects functor $J\from \C \to \C'$.  
    For example, if $\C$ is a category of games and strategies, $\C'$ might be a category whose objects are the same games as $\C$, but where the strategies are less rigidly constrained.
  \item Prove that the model $\C'$ is a Cartesian closed category and is computationally adequate for $\L'$.
  \item Prove a factorization result that exhibits every morphism $g$ in $\C'$ as the composition of some morphism $Jf$ in the image of $J$ with one of some fixed collection of morphisms that are known to be definable in $\L'$ (for example, single terms from $\L'\setminus\L$).
    In addition, if $g$ is compact, then $f$ should be compact in $\C$.
\end{itemize}
The third bullet point allows us to deduce a compact definability result for $\L'$ in $\C'$ from the compact definability result of $\L$ in $\C$, and then Full Abstraction follows as we have outlined in Section \ref{SecCAnFA}.

The proofs of Full Abstraction for stateful and nondeterministic languages that we have mentioned \cite{SamsonGuyIAActive,mcCHFiniteND} prove Full Abstraction in this way.  
Other important Full Abstraction results in Game Semantics that follow this pattern include Danos and Harmer's result for a probabilistic variant of Idealized Algol \cite{DanosHarmer}, the Full Abstraction result for general references of Abramsky, Honda and McCusker \cite{HondaMcCusker}, Laird's result for local exceptions \cite{LLi}, Murawski and Tzevelekos's Nominal Game Semantics \cite{NGS} and the result for countable nondeterminism by Laird and the present author \cite{CslPaper}.

Lastly, it is worth mentioning several papers that prove Full Abstraction without going through a factorization result.  
This is usually because they depart more radically from traditional game semantics.  
Such papers include Tsukada and Ong's sheaf-based models for nondeterministic PCF \cite{TsukadaSheaves,Sheaves}, Laird's categorical semantics for coroutines \cite{FunctionalProgramsAsCoroutines} and the results for probabilistic PCF by Castellan, Clairambault, Paquet and Winskel using concurrent games \cite{PPCF}.

\section{Full Abstraction for Kleisli Categories}

One thing which these Full Abstraction results have in common is that they rely on some degree of human intuition to build the original model.  
This process can often be very difficult.  
For instance, it seems obvious that if we want to model a nondeterministic programming language, then we need to relax the determinism constraint on strategies.  
However, relaxing the determinism constraint turns out not to be enough on its own: recall that we model a nonterminating computation in game semantics by a strategy that has no $P$-reply to a particular $O$-move.  
How then do we model a term which chooses between terminating at a value $v$ and not terminating, and how do we distinguish it from a term which always terminates at $v$?  
In both cases, player $P$ has the reply $v$ to player $O$'s initial move, but now there is nothing to indicate the possibility of non-termination.  

Harmer and McCusker are able to solve this problem, in the finite nondeterminism case, by separately keeping track of divergences in the strategy, but further problems arise when we start to consider countable nondeterminism.  
Since Game Semantics usually keeps track of finite sequences of moves, nondeterministic strategies are unable to distinguish between a program that nondeterministically chooses a number $n$ and prints ``Hello, world'' $n$ times, and a program that either does the same thing, or prints out the message infinitely many times.
So now we have to add extra information in about infinite sequences of moves (see Levy's work on infinite trace equivalence \cite{LevyGsInfinite} and the work of Laird and the present author on nondeterministic Idealized Algol \cite{CslPaper}).

Things get even more difficult when we consider nondeterministic versions of stateless languages such as PCF.  
Naively relaxing the determinism constraint on the usual definition of an innocent strategy does not give the right notion of a nondeterministic innocent strategy \cite{TsukadaSheaves}.  
There are definitions of nondeterministic innocence due to Levy \cite{levy2014morphisms}, toTsukada and Ong \cite{TsukadaSheaves}, and, via concurrent games, to Castellan, Clairambault, Hayman and Winskel \cite{NonAngelic}, that use the concept of \emph{morphisms between plays}, but these are already some distance removed conceptually from Hyland and Ong's original paper that introduced deterministic innocent strategies.
Tsukada and Ong's paper involves a complete recasting of strategies as sheaves in order to understand what happens when we try to mix nondeterminism and innocence.

This is just one example that shows that the process of adding different computational effects into a game semantics can be very hard and can require some ingenuity.
Meanwhile, there is plenty of well-known theory that deals with computational effects in a purely systematic way.  
We have already met two such techniques in section \ref{SecCatSem}: monads (and Kleisli categories) and Lawvere theories.
Put simply, the purpose of this thesis is to investigate what happens when we try to use these systematic techniques to prove Full Abstraction results for effectful languages.

Let us start by looking at Kleisli categories for monads.  
Recall that a monad $M$ on a category $\C$ is a functor $M \from \C \to \C$ that satisfies certain conditions, and that the Kleisli category $\Kl_M\C$ of $M$ has the objects of $\C$ as its objects, and that a morphism $a \to b$ in $\Kl_M\C$ is given by a morphism $a \to Mb$ in $\C$.
There is a natural identity-on-objects functor $J\from \C \to \Kl_M\C$.

In particular, if $a$ is any object of $\C$, then we have a distinguished Kleisli morphism $\phi_A\from Ma \to a$ in $\Kl_M\C$ given by the identity morphism $Ma \to Ma$ in $\C$.
Now let $f\from a \to b$ be an arbitrary morphism in $\Kl_M\C$, given by a morphism $\hat f\from a \to Mb$ in $\C$.
Then we can write $f$ as the following composite in $\Kl_M\C$.
\[
  a \xrightarrow{J\hat{f}} Mb \xrightarrow{\phi_b} b
  \]
In other words, the Kleisli category $\Kl_M\C$ automatically satisfies a factorization result of the type we have been talking about.  
So if $\C$ is a model of a language $\L$ that satisfies compact definability, then $\Kl_M\C$ will automatically satisfy compact definability for any denotational semantics for a language language $\L'$ that includes (via the functor $J$) the existing denotational semantics for $\L$ and in which the morphisms $\phi_a$ are all definable.

Proving computational adequacy is not so automatic.  
One approach is to treat $\Kl_M\C$ like any other model and to apply the usual arguments for proving adequacy.  
Most computational adequacy arguments rely on order-enriched properties of the underlying categories, particularly for dealing with recursion, and the Kleisli category $\Kl_M\C$ automatically inherits order-enriched structure from $\C$, by saying that $f\le g\from a \to b$ in $\Kl_M\C$ if $f\le g$ when considered as morphisms $a \to Mb$ in $\C$.

An alternative approach, which we shall use in this thesis, is to try and deduce computational adequacy for $\Kl_M\C$ from a computational adequacy result for the original language $\L$ in $\C$.  
In this approach, we take a term $N$ of the extended language $\L'$ and note that its denotation $\deno{N} \from a \to b$ in $\Kl_M\C$ is given by some morphism $f \from a \to Mb$ in $\C$.  
Typically, it is easy to construct a term $P$ of $\L$ whose denotation in $\C$ is $f$.
We then prove results to peg the operational behaviour of $N$ in $\L'$ to that of $P$ in $\L$, allowing us to deduce a computational adequacy result for our model of $\L'$ from the corresponding result for $\L$.

The last ingredient that we need is to prove that $\Kl_M\C$ is a Cartesian closed category.  
This is not immediate either: $\Kl_M\C$ need not be Cartesian, even if $\C$ is.  
For the purposes of this thesis, we will be considering only monads of a very special form -- the \emph{reader monads} $R_z$ on Cartesian closed categories, given by $R_za = z \to a$.
The Kleisli category for a reader monad on a Cartesian closed category is always Cartesian closed \cite{FunctionalCompleteness}.

\section{Difficulties with Countable Nondeterminism}

This thesis will focus on nondeterministic effects.  
Of these, perhaps the trickiest to work with is countable nondeterminism, in which a program can nondeterministically choose between a possibly infinite number of different options, without the possibility of non-termination\footnote{If we have a source of finite nondeterminism, then we can get infinite branching if we accept the possibility that our program might never terminate -- for example, count the number of coin tosses it takes before we get the first head.}.

There are three main difficulties when dealing with countable nondeterminism in game semantics.  
We have covered the first in the previous section: we cannot tell everything about a program's behaviour by looking at its possible finite traces, even when the traces are allowed to be arbitrarily long, as in the example where a program nondeterministically chooses a number $n$ and prints out a message $n$ times vs the program which may print out the message infinitely many times.  

The fact that finite nondeterminism avoids this behaviour comes down to K\"{o}nig's Lemma, which asserts that any finitely branching tree without an infinite branch has bounded height.  

Related is second problem, which is the failure of continuity of composition.  
This was noticed by Dijkstra in \cite[Ch.~9]{DijkstraBook} and later studied by Plotkin and Apt in \cite{PlotkinApt}.

In order to explain what we mean by continuity, we define the \emph{observational preorder} on terms of a language.  
If $M,N\from T$, we write $M\lesssim N$ if for all ground-type contexts $C[-]$ with a hole of type $T$ we have
\[
  C[M]\text{ always terminates} \Rightarrow C[N]\text{ always terminates.}
  \]
In particular, $M$ and $N$ are observationally equivalent if and only if $M\lesssim N$ and $N \lesssim M$.

Continuity of composition means that least upper bounds with respect to this order are preserved by function application.  
Consider, for example, the infinite sequence of terms $\le_m \from \bN \to \bN$ for $m=0,1,\dots$ that return $0$ if their argument is less than or equal to $m$ and go into an infinite loop otherwise.
It is fairly clear (with a straightforward rigorous proof once we have introduced the denotational semantics) that a least upper bound for the $\le_m$ is the term $\le_\infty$ that evaluates its argument and then returns $0$ regardless of what value it finds\footnote{$\le_\infty$ differs slightly from $\lambda n.0$ -- also an upper bound for the $\le_m$, but not the least upper bound -- in that it evaluates its argument before returning $0$.  
This means that it will fail to terminate if its argument fails to terminate.}.

But now notice what happens if we have a countable nondeterminism primitive $?$ in our language which when evaluated returns an arbitrarily large natural number.
Then the terms $\le_m ?$ are all observationally equivalent: they either return $0$ (when $?$ returns a number less than or equal to $m$), or fail to terminate (when $?$ returns a number greater than $m$).
We might write
\[
  \le_m ? = 0 + \Omega\,.
  \]
Therefore, the sequence $\le_m?$ is a constant sequence and its least upper bound is that constant value $0+\Omega$.

Meanwhile, $\le_\infty?$ always returns $0$.  
Therefore, application to $?$ does not preserve least upper bounds.

The reason that this is a problem is that a typical strategy for proving computational adequacy is via an order-enriched category in which the observational preorder at a type matches up with the ordering of morphisms into the denotation of that type.  
The standard adequacy proof uses continuity of composition in the model in an essential way -- see Lemma \ref{LemAdequacyLimits} for an example of such an argument being used to prove computational adequacy for a deterministic language.
But if the ordering of morphisms in a denotational model of a language with countable nondeterminism faithfully respects the observational ordering of terms, then composition of morphisms is necessarily not continuous -- since it is not continuous in the language itself.

Our strategy for getting around this problem goes back to Levy's paper \emph{Infinite Trace Equivalence} \cite{LevyGsInfinite}, and essentially involves going via an auxiliary language, in which the nondeterministic oracle $?$, when it chooses a value, must print that value to a log.  
Given a term $P$ of ground type, we write $P\converges_u$ if $P$ terminates whenever it prints the sequence $u$ to the log.  
We then replace our earlier definition of the observational preorder with a stronger one: we write that $M\lesssim N$ if for all suitable contexts $C[-]$ we have
\[
  C[M] \converges_u \mathrel{\Rightarrow C[N]} \converges_u\,.
  \]
This ordering on terms is continuous.
For example, the extra requirement wrecks our previous example of failure of continuity of composition.  
Indeed, the sequence $\le_m ?$ is no longer constant, since, for example, $\le_5 ?$ does not terminate if it prints $6$ to the log, whereas $\le_{1000} ?$ does.
The least upper bound of the $\le_m?$ is then the term that terminates whenever it prints a single number to the log; i.e., $\le_\infty ?$.

We can prove continuoity of composition in general for the new language, which allows us to prove computational adequacy in the usual way.
Having done this, we then use operational methods to find a way to identify terms that should be observationally equivalent in the original sense.  
This allows us to quotient out our model for the new language to get one that is computationally adequate for the ordinary nondeterministic version.

The last problem is to do with definability.
First note that there are two different reasons why an element of a denotational model (e.g., a strategy) might not be definable in a particular language.  
One reason is structural: for example, a stateless language such as PCF cannot define a strategy whose behaviour depends on its entire history, since that would indicate stateful behaviour.  
The other reason is purely computational.  
Given a non-computable function $f\from \bN \to \bN$ (for example, the function that returns $0$ if its argument is source code for a terminating program and $1$ otherwise), we have a perfectly well-behaved history free strategy that represents $f$, but $f$ is nevertheless still not definable in most programming languages.  
Historically, the study of Full Abstraction has not been too concerned with non-definable elements of the second kind: the reason is that they do not usually play a role in determining the intrinsic equivalence relation.
However, in the presence of countable nondeterminism, there exist definable terms that can be distinguished in the model that cannot be distinguished by definable elements, but can be distinguished by non-computable functions.
An example of such a function is found in Proposition \ref{PropNotFullyAbstract}; the general idea is due to Kleene and can be found in \cite{KleeneTree}.

What this means is that if we want to model countable nondeterminism then structural constraints are not enough: we need to impose computability constraints on strategies as well.
Happily, such notions are well studied already, even appearing in the original Hyland-Ong and AJM papers \cite{hoPcf,ajmPcf}, in which the authors give a complete characterization of those strategies definable in PCF -- a \emph{universality} result for the model.  
Similar results can also be found in earlier work on Full Abstraction, such as Plotkin's original PCF paper \cite{PlotkinPcf}.

The difficulties with countable nondeterminism that we have outlined are essentially domain-theoretic, and can be summed up by saying that composition of countably nondeterministic functions is not continuous -- i.e., does not preserve least upper bounds -- with respect to the natural orderings on terms.

\section{Plan for this Thesis}

This thesis will develop the theory of Full Abstraction from the point of view of techniques of categorical algebra such as Kleisli categories.

\begin{description}
  \item[Chapters \ref{ChapGames} and \ref{ChapFullAbstraction}] give a fairly traditional presentation of a Fully Abstract game semantics model for Idealized Algol.  
    All subsequent results will be for extended versions of Idealized Algol, so this model can serve as the foundation for the rest of our work.  
    Although these chapters form by some distance the longest part of the thesis, they also contain the smallest amount of new material, and essentially cover the same material as Abramsky and McCusker's Fully Abstract game semantics for Idealized Algol \cite{SamsonGuyIAActive}.  
    The main difference, which ties in with the general theme of the thesis, is that we prove Computational Adequacy using techniques of categorical algebra rather than via the ad hoc combinatorial arguments found in \cite{SamsonGuyIAActive}.  
    Specifically, we use the concept of a \emph{sequoidal category} and of the exponential as a final coalgebra, introduced by Laird in \cite{laird02} to prove Full Abstraction for a language with general references.
    We present the first application of this technique to a language with purely local state of ground type.
    The remainder of the Full Abstraction result is largely as in \cite{SamsonGuyIAActive}.
  \item[Chapter \ref{ChapMonads}] presents the general theory of monads and Kleisli categories.  
    It then presents a technique which can be used to prove Full Abstraction for several nondeterministic effects, along the lines we outlined in the previous section.
\end{description}
For the remainder of the thesis, we deal with a generalization of monads -- \emph{parametric monads}, in which the action of the monad is parameterized by an object of some monoidal category $\X$ (so that we deal with a functor $\X \times \C \to \C$ rather than a functor $\C \to \C$).
\begin{description}
  \item[Chapter \ref{ChapParametricMonads}] introduces and defines parametric monads (also known as \emph{lax actions}), and proves a number of technical results which we need.
    It also introduces the \emph{\Mellies category} and the oplax $2$-limit $\C/\X$, which give us two related analogues of the Kleisli category in the parametric case.
    The \Mellies category is an enriched category, while $\C/\X$ is an ordinary category obtained from it by change of base.
    As with monads, we need to specialize to a small class of parametric monads -- the \emph{reader actions} -- in order to ensure that $\C/\X$ is a Cartesian closed category.
  \item[Chapter \ref{ChapReaderActions}] uncovers a large source of these reader actions; in this chapter, we prove that reader actions on the category $\Set$ of sets are equivalent to lax monoidal functors $\Set \to \Set$.
  \item[Chapter \ref{ChapPromonads}] takes a small detour away from the main narrative to study the \Mellies category from the point of view of profunctors.
  \item[Chapter \ref{ChapParametricMonadsFullAbstraction}] completes our study of parametric monads by proving a Full Abstraction result for categories derived from \Mellies categories, using similar techniques to Chapter \ref{ChapMonads}.
    As an example, we give a semantics for a probabilistic language for a category derived from a particular parametric monad, and show that it is Fully Abstract.
\end{description}
Lastly, our conclusion in {\bf Chapter \ref{ChapFurtherDirections}} provides a glimpse into several further directions that are left unexplored by the rest of the thesis.
